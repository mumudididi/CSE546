{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    mndata = MNIST('/home/mumu/Desktop/CSE546/hw2/data/python-mnist/data')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    X_test, labels_test = map(np.array, mndata.load_testing())\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "    return X_test,labels_test,X_train,labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(vector):\n",
    "    n_classes = len(vector.unique())  # 1\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 2\n",
    "    return one_hot\\\n",
    "        .scatter(1, vector.type(torch.LongTensor).unsqueeze(1), 1)  # 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,labels_test,X_train,labels_train=load_dataset()\n",
    "y_train = torch.from_numpy(labels_train)\n",
    "y_one_hot = one_hot_encode(y_train)\n",
    "n,d = X_train.shape\n",
    "X_train_tensor = torch.from_numpy(X_train).double()\n",
    "W = torch.rand(d, 10, requires_grad=True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "################################ Multinomial Logistic Regression ###########################################\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max of the 0 the iteration is 0.06373168528079987\n",
      "the max of the 100 the iteration is 0.010296175256371498\n",
      "the max of the 200 the iteration is 0.005794010125100613\n",
      "the max of the 300 the iteration is 0.004077061545103788\n",
      "the max of the 400 the iteration is 0.0031427759677171707\n",
      "the max of the 500 the iteration is 0.002765779849141836\n",
      "the max of the 600 the iteration is 0.0025410777889192104\n",
      "the max of the 700 the iteration is 0.002359171863645315\n",
      "the max of the 800 the iteration is 0.002207828452810645\n",
      "the max of the 900 the iteration is 0.002079244237393141\n",
      "the max of the 1000 the iteration is 0.00196815631352365\n",
      "the max of the 1100 the iteration is 0.0018708654679358006\n",
      "the max of the 1200 the iteration is 0.001784683670848608\n",
      "the max of the 1300 the iteration is 0.00170760543551296\n",
      "the max of the 1400 the iteration is 0.0016381009481847286\n",
      "the max of the 1500 the iteration is 0.00157497962936759\n",
      "the max of the 1600 the iteration is 0.0015172993298619986\n",
      "the max of the 1700 the iteration is 0.0014643047470599413\n",
      "the max of the 1800 the iteration is 0.0014153801603242755\n",
      "the max of the 1900 the iteration is 0.001370019861496985\n",
      "the max of the 2000 the iteration is 0.001327802543528378\n",
      "the max of the 2100 the iteration is 0.0012883737217634916\n",
      "the max of the 2200 the iteration is 0.0012514350237324834\n",
      "the max of the 2300 the iteration is 0.0012167301028966904\n",
      "the max of the 2400 the iteration is 0.0011840392835438251\n",
      "the max of the 2500 the iteration is 0.0011531729251146317\n",
      "the max of the 2600 the iteration is 0.0011239650193601847\n",
      "the max of the 2700 the iteration is 0.0010962699307128787\n",
      "the max of the 2800 the iteration is 0.001069961697794497\n",
      "the max of the 2900 the iteration is 0.001044926350004971\n",
      "the max of the 3000 the iteration is 0.0010210640029981732\n",
      "tensor(0.0007)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "epochs=100000\n",
    "MLR_w_autograd = torch.zeros((d, 10), requires_grad=True)\n",
    "step_size = 0.1\n",
    "for epoch in range(epochs):\n",
    "    y_hat = torch.matmul(X_train_tensor, MLR_w_autograd.double())\n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(y_hat, y_train.long())\n",
    "    # computes derivatives of the loss with respect to W\n",
    " \n",
    "    loss.backward()\n",
    "    #print(torch.max(w_autograd.grad))\n",
    "    MLR_w_autograd.data = MLR_w_autograd.data - step_size * MLR_w_autograd.grad\n",
    "    \n",
    "    if (torch.max(torch.abs((MLR_w_autograd.grad))) < 0.001):\n",
    "        print(torch.max(MLR_w_autograd.grad))\n",
    "        print(MLR_w_autograd)\n",
    "        break\n",
    "    if (epoch % 100 == 0):\n",
    "        print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(MLR_w_autograd.grad))))\n",
    "    \n",
    "    MLR_w_autograd.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 10])\n",
      "torch.Size([60000, 784])\n",
      "60000\n",
      "\n",
      "Final Train set  Accuracy for Multinomial Regression is : 0.9151833333333333\n"
     ]
    }
   ],
   "source": [
    "############################# PREDICT FOR MULTINOMIAL REG ######################################\n",
    "print(MLR_w_autograd.shape)\n",
    "print(X_train_tensor.shape)\n",
    "predict = torch.matmul(X_train_tensor.double(),MLR_w_autograd.double())\n",
    "MLR_predicted_labels_train = torch.argmax(predict,axis=1)\n",
    "print(labels_train.shape[0])\n",
    "MLR_train_accuracy = float(sum(MLR_predicted_labels_train== torch.from_numpy(labels_train))) / labels_train.shape[0]\n",
    "print(\"\\nFinal Train set  Accuracy for Multinomial Regression is : {}\".format(MLR_train_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test set Accuracy for Multinomial Regression is : 0.9182\n"
     ]
    }
   ],
   "source": [
    "############################# PREDICT FOR MULTINOMIAL REG ######################################\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "#print(MLR_w_autograd.shape)\n",
    "#print(X_test_tensor.shape)\n",
    "predict_test = torch.matmul(X_test_tensor.double(),MLR_w_autograd.double())\n",
    "MLR_predicted_labels_test = torch.argmax(predict_test,axis=1)\n",
    "#print(MLR_predicted_labels_test.shape)\n",
    "MLR_train_accuracy_test = float(sum(MLR_predicted_labels_test== torch.from_numpy(labels_test))) / labels_test.shape[0]\n",
    "print(\"\\nFinal test set Accuracy for Multinomial Regression is : {}\".format(MLR_train_accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "###################### MSE loss ##################################################\n",
    "################################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max of the 0 the iteration is 0.021706614643335342\n",
      "the max of the 100 the iteration is 0.0017530462937429547\n",
      "the max of the 200 the iteration is 0.0011112616630271077\n",
      "tensor(0.0007)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "epochs=10000\n",
    "MSE_w_autograd = torch.zeros((d, 10), requires_grad=True)\n",
    "step_size = 0.1\n",
    "for epoch in range(epochs):\n",
    "    y_hat = torch.matmul(X_train_tensor, MSE_w_autograd.double())\n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(y_hat.double(), y_one_hot.double())\n",
    "    # computes derivatives of the loss with respect to W\n",
    " \n",
    "    loss.backward()\n",
    "    #print(torch.max(w_autograd.grad))\n",
    "    MSE_w_autograd.data = MSE_w_autograd.data - step_size * MSE_w_autograd.grad\n",
    "    if (torch.max(torch.abs((MSE_w_autograd.grad))) < 0.001):\n",
    "        print(torch.max(MSE_w_autograd.grad))\n",
    "        print(MSE_w_autograd)\n",
    "        break\n",
    "    if (epoch % 100 == 0):\n",
    "        print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(MSE_w_autograd.grad))))\n",
    "    \n",
    "    MSE_w_autograd.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 10])\n",
      "torch.Size([60000, 784])\n",
      "60000\n",
      "\n",
      "Final Trainning set Accuracy for MSE is : 0.8448666666666667\n"
     ]
    }
   ],
   "source": [
    "############################# PREDICT FOR MULTINOMIAL REG ######################################\n",
    "print(MSE_w_autograd.shape)\n",
    "print(X_train_tensor.shape)\n",
    "predict = torch.matmul(X_train_tensor.double(),MSE_w_autograd.double())\n",
    "MSE_predicted_labels_train = torch.argmax(predict,axis=1)\n",
    "print(labels_train.shape[0])\n",
    "MSE_test_accuracy = float(sum(MSE_predicted_labels_train== torch.from_numpy(labels_train))) / labels_train.shape[0]\n",
    "print(\"\\nFinal Trainning set Accuracy for MSE is : {}\".format(MSE_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test set Accuracy for Least square is : 0.8519\n"
     ]
    }
   ],
   "source": [
    "############################# PREDICT FOR MULTINOMIAL REG ######################################\n",
    "predict_test = torch.matmul(X_test_tensor.double(),MSE_w_autograd.double())\n",
    "MSE_predicted_labels_test = torch.argmax(predict_test,axis=1)\n",
    "#print(MLR_predicted_labels_test.shape)\n",
    "MSE_train_accuracy_test = float(sum(MSE_predicted_labels_test== torch.from_numpy(labels_test))) / labels_test.shape[0]\n",
    "print(\"\\nFinal test set Accuracy for Least square is : {}\".format(MSE_train_accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

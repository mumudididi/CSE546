{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt    \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    z should be a scaler\n",
    "    '''\n",
    "    return 1/(1+np.exp(-z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta,X,y,Lambda):\n",
    "    '''\n",
    "    X is a (n,d) matrix\n",
    "    y is a (n,)vector\n",
    "    w is a (n+1,) => (b,w1,w2,w3,....)\n",
    "    Lambda is a scalar\n",
    "    '''\n",
    "    n,d = X.shape\n",
    "    X_tilda =np.c_[np.ones((n,1)),X]\n",
    "    J = np.zeros((n,))\n",
    "    for i in range(n):\n",
    "        # J should be (n,)\n",
    "        J[i] = np.log(1 + np.exp(-y[i] * (X_tilda[i,:] @ theta)))\n",
    "    \n",
    "    reg = Lambda * theta[1:].T @ theta[1:]\n",
    "    return 1/n * np.sum(J) + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradF(theta,X,y,Lambda):\n",
    "    '''\n",
    "    X is a (n,d) matrix\n",
    "    y is a (n,)vector\n",
    "    w is a (n+1,) => (b,w1,w2,w3,....)\n",
    "    Lambda is a scalar\n",
    "    '''\n",
    "        \n",
    "    n,d = X.shape\n",
    "    X_tilda =np.c_[np.ones((n,1)),X]\n",
    "#    mu = np.zeros((n,))\n",
    "#     for i in range(d):\n",
    "#         mu[i] = sigmoid(y[i] * X[i,:] @ w)\n",
    "    #mu should be a n-by-1\n",
    "    mu = 1/(1+ np.exp(np.multiply(-y.T , (X_tilda @ theta)))) \n",
    "    del_J = np.zeros(d+1)\n",
    "    # derivative wrt. d \n",
    "    del_J[0] =  1/n * (1-mu).T @ (-y) \n",
    "    # derivative wrt w=(w0,w1,w2...)\n",
    "    for i in range(1,d+1):\n",
    "        del_J[i] = 1/n * (1-mu).T @ (-np.multiply(y,X_tilda[:,i])) + 2*Lambda*theta[i]\n",
    "    return del_J\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(theta_init,gradient,step_size,tolerence):\n",
    "    theta_list = [theta_init]\n",
    "    theta = theta_init\n",
    "    curr_grad = gradient(theta)\n",
    "    num_iteration = 0\n",
    "    #print(\"curr gradient is {}\".format(np.max(np.abs(curr_grad))))\n",
    "    while(np.max(np.abs(curr_grad)) > tolerence and num_iteration < 100000):\n",
    "        #print(\"************************************************************\")\n",
    "        #print(\"curr gradient is {}\".format(np.max(np.abs(curr_grad))))\n",
    "        #print(\"current iteration {}\" .format(num_iteration))\n",
    "        num_iteration += 1  \n",
    "        theta = theta - step_size*curr_grad\n",
    "        theta_list.append(theta)\n",
    "        curr_grad = gradient(theta)\n",
    "    if (num_iteration == 10000):\n",
    "         error(\"not converging\")\n",
    "    print(\"it takes {0} iterations to converge\".format(num_iteration))\n",
    "    return theta,theta_list,num_iteration\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_gradF(theta,X,y,Lambda,m):\n",
    "    '''\n",
    "    X is a (n,d) matrix\n",
    "    y is a (n,)vector\n",
    "    w is a (n+1,) => (b,w1,w2,w3,....)\n",
    "    Lambda is a scalar\n",
    "    m is the number of batches, m <= n\n",
    "    '''\n",
    "        \n",
    "    n,d = X.shape\n",
    "    selector = random.sample(range(0,n), m) # max is n-1\n",
    "    \n",
    "    X_tilda =np.c_[np.ones((m,1)),np.copy(X[selector, :])]\n",
    "    y_tilda = np.copy(y[selector]) \n",
    "    \n",
    "    mu = 1/(1+ np.exp(np.multiply(-y_tilda.T , (X_tilda @ theta)))) \n",
    "    del_J = np.zeros(d+1)\n",
    "    \n",
    "    del_J[0] =  1/m * (1-mu).T @ (-y_tilda) \n",
    "    \n",
    "    for i in range(1,d+1):\n",
    "        del_J[i] = 1/m * (1-mu).T @ (-np.multiply(y_tilda,X_tilda[:,i])) + 2*Lambda*theta[i]\n",
    "    return del_J\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

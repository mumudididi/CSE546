{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    mndata = MNIST('/home/mumu/Desktop/CSE546/hw1/python-mnist/data/')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    X_test, labels_test = map(np.array, mndata.load_testing())\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "    return X_test,labels_test,X_train,labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector):\n",
    "    n_classes = len(vector.unique())  # 1\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 2\n",
    "    return one_hot\\\n",
    "        .scatter(1, vector.type(torch.LongTensor).unsqueeze(1), 1)  # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,labels_test,X_train,labels_train=load_dataset()\n",
    "y_train = torch.from_numpy(labels_train)\n",
    "y_one_hot = one_hot_encode(y_train)\n",
    "X_train_tensor = torch.from_numpy(X_train).double()\n",
    "n_train,d_train = X_train_tensor.shape\n",
    "#W = torch.rand(d, 10, requires_grad=True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([785, 64])\n",
      "torch.Size([60000, 785])\n",
      "torch.Size([65, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h1 = 64 \n",
    "h2 = 10\n",
    "# Wx+b => W -> [w_0 W], x -> [1;x]\n",
    "W1 = 2/np.sqrt(h1) * torch.rand( h1 , 1+d_train ).T.double() -  1/np.sqrt(h1) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "W2 = 2/np.sqrt(h2) * torch.rand( h1+1 , h2 ).double() -  1/np.sqrt(h2) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "# adding columns of 1 to x\n",
    "ones = torch.ones(n_train,1).double()\n",
    "X_train_padded =torch.cat((ones,X_train_tensor),1)\n",
    "print(W1.shape)\n",
    "print(X_train_padded.shape)\n",
    "# 60000, 64 -> 60000,65  -> 65, 10 \n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 64])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "h1 = 64 \n",
    "h2 = 10\n",
    "# Wx+b => W -> [w_0 W], x -> [1;x] \n",
    "W1 = 2/np.sqrt(h1) * torch.rand( h1 , d_train ).T.double() -  1/np.sqrt(h1) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b1 = ones = torch.ones(n_train,1).double()\n",
    "W2 = 2/np.sqrt(h2) * torch.rand( h1 , h2 ).double() -  1/np.sqrt(h2) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b2 = ones = torch.ones(n_train,1).double()\n",
    "\n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### separate W and b ######################################\n",
    "epochs=1000\n",
    "learning_rate = .01\n",
    "parms = [W1,b1,W2,b2]#list(list(W2)) #+list(W2))\n",
    "optimizer = torch.optim.Adam(parms, lr=learning_rate)\n",
    "W1.requires_grad = True\n",
    "W2.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "b2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,\t3.84\n",
      "the max of the 0 the iteration is 0.14327158906478363\n",
      "the max of the 0 the iteration is 0.869948240131642\n",
      "10,\t0.77\n",
      "the max of the 10 the iteration is 0.018864492205162692\n",
      "the max of the 10 the iteration is 0.13327565576751776\n",
      "20,\t0.46\n",
      "the max of the 20 the iteration is 0.006056820062351494\n",
      "the max of the 20 the iteration is 0.0699225336010189\n",
      "30,\t0.36\n",
      "the max of the 30 the iteration is 0.004001705342024019\n",
      "the max of the 30 the iteration is 0.04761287633369297\n",
      "40,\t0.29\n",
      "the max of the 40 the iteration is 0.0020758061867835707\n",
      "the max of the 40 the iteration is 0.027255358354347146\n",
      "50,\t0.25\n",
      "the max of the 50 the iteration is 0.0018502683665875385\n",
      "the max of the 50 the iteration is 0.015207029549459085\n",
      "60,\t0.22\n",
      "the max of the 60 the iteration is 0.0011865689892031734\n",
      "the max of the 60 the iteration is 0.010482282422754757\n",
      "70,\t0.20\n",
      "the max of the 70 the iteration is 0.0009335415333422896\n",
      "the max of the 70 the iteration is 0.008977343282477528\n",
      "80,\t0.18\n",
      "the max of the 80 the iteration is 0.0008694547501260709\n",
      "the max of the 80 the iteration is 0.007263523200744989\n",
      "90,\t0.16\n",
      "the max of the 90 the iteration is 0.0007102198344643257\n",
      "the max of the 90 the iteration is 0.004448978284503251\n",
      "100,\t0.14\n",
      "the max of the 100 the iteration is 0.0006254628416434797\n",
      "the max of the 100 the iteration is 0.00305822382445059\n",
      "110,\t0.13\n",
      "the max of the 110 the iteration is 0.0005892659286632363\n",
      "the max of the 110 the iteration is 0.0029727596470378237\n",
      "120,\t0.11\n",
      "the max of the 120 the iteration is 0.0005530508746022563\n",
      "the max of the 120 the iteration is 0.0029127836698793217\n",
      "130,\t0.10\n",
      "the max of the 130 the iteration is 0.000523853144513815\n",
      "the max of the 130 the iteration is 0.0025388080149697993\n",
      "140,\t0.09\n",
      "the max of the 140 the iteration is 0.0004944316065717479\n",
      "the max of the 140 the iteration is 0.0023153668044060558\n",
      "150,\t0.09\n",
      "the max of the 150 the iteration is 0.00045672038943756377\n",
      "the max of the 150 the iteration is 0.002100316472625179\n",
      "160,\t0.08\n",
      "the max of the 160 the iteration is 0.00042185043501273496\n",
      "the max of the 160 the iteration is 0.001889669827155672\n",
      "170,\t0.07\n",
      "the max of the 170 the iteration is 0.0003843288236998692\n",
      "the max of the 170 the iteration is 0.001686415018783225\n",
      "180,\t0.07\n",
      "the max of the 180 the iteration is 0.0003504592003291758\n",
      "the max of the 180 the iteration is 0.0015385927602188695\n",
      "190,\t0.06\n",
      "the max of the 190 the iteration is 0.00032308857096902116\n",
      "the max of the 190 the iteration is 0.0014858783190920362\n",
      "200,\t0.06\n",
      "the max of the 200 the iteration is 0.0002988995653142545\n",
      "the max of the 200 the iteration is 0.001356792895794314\n",
      "210,\t0.06\n",
      "the max of the 210 the iteration is 0.0002796359303498213\n",
      "the max of the 210 the iteration is 0.0013291829768875745\n",
      "220,\t0.05\n",
      "the max of the 220 the iteration is 0.0002562408305428897\n",
      "the max of the 220 the iteration is 0.0012982308798664356\n",
      "230,\t0.05\n",
      "the max of the 230 the iteration is 0.00023665710912821534\n",
      "the max of the 230 the iteration is 0.0013146137077358856\n",
      "240,\t0.05\n",
      "the max of the 240 the iteration is 0.00022079311483761977\n",
      "the max of the 240 the iteration is 0.001211367224813232\n",
      "250,\t0.04\n",
      "the max of the 250 the iteration is 0.00020826355715695917\n",
      "the max of the 250 the iteration is 0.0011494849366639492\n",
      "260,\t0.04\n",
      "the max of the 260 the iteration is 0.00020857332509825104\n",
      "the max of the 260 the iteration is 0.0010930689669274823\n",
      "270,\t0.04\n",
      "the max of the 270 the iteration is 0.00019704983620988937\n",
      "the max of the 270 the iteration is 0.001109611498569467\n",
      "280,\t0.04\n",
      "the max of the 280 the iteration is 0.00018662473079731584\n",
      "the max of the 280 the iteration is 0.0011091904243844984\n",
      "290,\t0.03\n",
      "the max of the 290 the iteration is 0.0001759581149959278\n",
      "the max of the 290 the iteration is 0.0010607540603147362\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MLR_w_autograd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-c1a1151dfbbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the max of the {} the iteration is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLR_w_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLR_w_autograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLR_w_autograd' is not defined"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "for epoch in range(epochs):\n",
    "    # fc1: #observation-by-64 \n",
    "    fc1 = F.relu (torch.matmul(X_train_tensor, W1.double()) + b1 ) \n",
    "\n",
    "    y_hat =torch.matmul(fc1, W2.double())+b2\n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(y_hat, y_train.long())\n",
    "    \n",
    "    # before loss.backwards()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0 :\n",
    "        print('{},\\t{:.2f}'.format(epoch, loss.item()))\n",
    "        print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W1.grad))))\n",
    "        print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W2.grad))))\n",
    "    if ((torch.max(torch.abs((W1.grad))) < 0.001 ) and torch.max(torch.abs((W2.grad))) < 0.001 ) :\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "learning_rate = 1\n",
    "parms = list(W1)+list(W2)#list(list(W2)) #+list(W2))\n",
    "optimizer = torch.optim.Adam(parms, lr=learning_rate)\n",
    "W1.requires_grad = True\n",
    "W2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,\t2.47\n",
      "the max of the 0 the iteration is 0.10749878708497858\n",
      "1,\t2.47\n",
      "the max of the 1 the iteration is 0.21499757416995716\n",
      "2,\t2.47\n",
      "the max of the 2 the iteration is 0.32249636125493575\n",
      "3,\t2.47\n",
      "the max of the 3 the iteration is 0.4299951483399143\n",
      "4,\t2.47\n",
      "the max of the 4 the iteration is 0.5374939354248929\n",
      "5,\t2.47\n",
      "the max of the 5 the iteration is 0.6449927225098715\n",
      "6,\t2.47\n",
      "the max of the 6 the iteration is 0.7524915095948501\n",
      "7,\t2.47\n",
      "the max of the 7 the iteration is 0.8599902966798287\n",
      "8,\t2.47\n",
      "the max of the 8 the iteration is 0.9674890837648074\n",
      "9,\t2.47\n",
      "the max of the 9 the iteration is 1.074987870849786\n",
      "10,\t2.47\n",
      "the max of the 10 the iteration is 1.1824866579347646\n",
      "11,\t2.47\n",
      "the max of the 11 the iteration is 1.2899854450197432\n",
      "12,\t2.47\n",
      "the max of the 12 the iteration is 1.3974842321047218\n",
      "13,\t2.47\n",
      "the max of the 13 the iteration is 1.5049830191897005\n",
      "14,\t2.47\n",
      "the max of the 14 the iteration is 1.612481806274679\n",
      "15,\t2.47\n",
      "the max of the 15 the iteration is 1.7199805933596577\n",
      "16,\t2.47\n",
      "the max of the 16 the iteration is 1.8274793804446363\n",
      "17,\t2.47\n",
      "the max of the 17 the iteration is 1.934978167529615\n",
      "18,\t2.47\n",
      "the max of the 18 the iteration is 2.0424769546145933\n",
      "19,\t2.47\n",
      "the max of the 19 the iteration is 2.149975741699572\n",
      "20,\t2.47\n",
      "the max of the 20 the iteration is 2.2574745287845506\n",
      "21,\t2.47\n",
      "the max of the 21 the iteration is 2.364973315869529\n",
      "22,\t2.47\n",
      "the max of the 22 the iteration is 2.472472102954508\n",
      "23,\t2.47\n",
      "the max of the 23 the iteration is 2.5799708900394864\n",
      "24,\t2.47\n",
      "the max of the 24 the iteration is 2.687469677124465\n",
      "25,\t2.47\n",
      "the max of the 25 the iteration is 2.7949684642094437\n",
      "26,\t2.47\n",
      "the max of the 26 the iteration is 2.9024672512944223\n",
      "27,\t2.47\n",
      "the max of the 27 the iteration is 3.009966038379401\n",
      "28,\t2.47\n",
      "the max of the 28 the iteration is 3.1174648254643795\n",
      "29,\t2.47\n",
      "the max of the 29 the iteration is 3.224963612549358\n",
      "30,\t2.47\n",
      "the max of the 30 the iteration is 3.3324623996343368\n",
      "31,\t2.47\n",
      "the max of the 31 the iteration is 3.4399611867193154\n",
      "32,\t2.47\n",
      "the max of the 32 the iteration is 3.547459973804294\n",
      "33,\t2.47\n",
      "the max of the 33 the iteration is 3.6549587608892726\n",
      "34,\t2.47\n",
      "the max of the 34 the iteration is 3.7624575479742512\n",
      "35,\t2.47\n",
      "the max of the 35 the iteration is 3.86995633505923\n",
      "36,\t2.47\n",
      "the max of the 36 the iteration is 3.9774551221442085\n",
      "37,\t2.47\n",
      "the max of the 37 the iteration is 4.084953909229187\n",
      "38,\t2.47\n",
      "the max of the 38 the iteration is 4.192452696314165\n",
      "39,\t2.47\n",
      "the max of the 39 the iteration is 4.299951483399143\n",
      "40,\t2.47\n",
      "the max of the 40 the iteration is 4.407450270484121\n",
      "41,\t2.47\n",
      "the max of the 41 the iteration is 4.514949057569099\n",
      "42,\t2.47\n",
      "the max of the 42 the iteration is 4.6224478446540775\n",
      "43,\t2.47\n",
      "the max of the 43 the iteration is 4.729946631739056\n",
      "44,\t2.47\n",
      "the max of the 44 the iteration is 4.837445418824034\n",
      "45,\t2.47\n",
      "the max of the 45 the iteration is 4.944944205909012\n",
      "46,\t2.47\n",
      "the max of the 46 the iteration is 5.05244299299399\n",
      "47,\t2.47\n",
      "the max of the 47 the iteration is 5.159941780078968\n",
      "48,\t2.47\n",
      "the max of the 48 the iteration is 5.267440567163947\n",
      "49,\t2.47\n",
      "the max of the 49 the iteration is 5.374939354248925\n",
      "50,\t2.47\n",
      "the max of the 50 the iteration is 5.482438141333903\n",
      "51,\t2.47\n",
      "the max of the 51 the iteration is 5.589936928418881\n",
      "52,\t2.47\n",
      "the max of the 52 the iteration is 5.697435715503859\n",
      "53,\t2.47\n",
      "the max of the 53 the iteration is 5.8049345025888375\n",
      "54,\t2.47\n",
      "the max of the 54 the iteration is 5.912433289673816\n",
      "55,\t2.47\n",
      "the max of the 55 the iteration is 6.019932076758794\n",
      "56,\t2.47\n",
      "the max of the 56 the iteration is 6.127430863843772\n",
      "57,\t2.47\n",
      "the max of the 57 the iteration is 6.23492965092875\n",
      "58,\t2.47\n",
      "the max of the 58 the iteration is 6.342428438013728\n",
      "59,\t2.47\n",
      "the max of the 59 the iteration is 6.4499272250987065\n",
      "60,\t2.47\n",
      "the max of the 60 the iteration is 6.557426012183685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b4ad4179ab9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# computes derivatives of the loss with respect to W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # fc1: #observation-by-64 \n",
    "    fc1 = F.relu (torch.matmul(X_train_padded, W1.double()) ) \n",
    "    ones = torch.ones(n_train,1).double()\n",
    "    # fc1_padded: #observation-by-65\n",
    "#     print('fc1 shape is {} '.format(fc1.shape))\n",
    "#     print('ones shape is {} '.format(ones.shape))\n",
    "    fc1_padded = torch.cat((ones,fc1),1)\n",
    "    y_hat =torch.matmul(fc1_padded, W2.double()) \n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(y_hat, y_train.long())\n",
    "    \n",
    "    # before loss.backwards()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print('{},\\t{:.2f}'.format(epoch, loss.item()))\n",
    "    print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W2.grad))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fc1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-e0252ad91249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fc1' is not defined"
     ]
    }
   ],
   "source": [
    "fc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

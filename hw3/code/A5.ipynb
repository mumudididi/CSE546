{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    mndata = MNIST('/home/mumu/Desktop/CSE546/hw1/python-mnist/data/')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    X_test, labels_test = map(np.array, mndata.load_testing())\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "    return X_test,labels_test,X_train,labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector):\n",
    "    n_classes = len(vector.unique())  # 1\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes))\\\n",
    "        .type(torch.LongTensor)  # 2\n",
    "    return one_hot\\\n",
    "        .scatter(1, vector.type(torch.LongTensor).unsqueeze(1), 1)  # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,labels_test,X_train,labels_train=load_dataset()\n",
    "y_train = torch.from_numpy(labels_train)\n",
    "y_test = torch.from_numpy(labels_test)\n",
    "\n",
    "y_one_hot = one_hot_encode(y_train)\n",
    "X_train_tensor = torch.from_numpy(X_train).double()\n",
    "n_train,d_train = X_train_tensor.shape\n",
    "X_test_tensor = torch.from_numpy(X_test).double()\n",
    "n_test,d_test = X_test_tensor.shape\n",
    "\n",
    "#W = torch.rand(d, 10, requires_grad=True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 64])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "h1 = 64 \n",
    "h2 = 10\n",
    "# Wx+b => W -> [w_0 W], x -> [1;x] \n",
    "W1 = 2/np.sqrt(h1) * torch.rand( h1 , d_train ).T.double() -  1/np.sqrt(h1) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b1 = ones = torch.ones(h1,1).double()\n",
    "W2 = 2/np.sqrt(h2) * torch.rand( h1 , h2 ).double() -  1/np.sqrt(h2) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b2 = ones = torch.ones(h2,1).double()\n",
    "\n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### separate W and b ######################################\n",
    "epochs=5000\n",
    "learning_rate = .01\n",
    "parms = [W1,b1,W2,b2]#list(list(W2)) #+list(W2))\n",
    "optimizer = torch.optim.Adam(parms, lr=learning_rate)\n",
    "W1.requires_grad = True\n",
    "W2.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "b2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is 0.09795\n",
      "training accuracy is 0.7749166666666667\n",
      "training accuracy is 0.8702666666666666\n",
      "training accuracy is 0.89285\n",
      "training accuracy is 0.9081333333333333\n",
      "training accuracy is 0.91885\n",
      "training accuracy is 0.9261833333333334\n",
      "training accuracy is 0.9312333333333334\n",
      "training accuracy is 0.9356833333333333\n",
      "training accuracy is 0.9391666666666667\n",
      "training accuracy is 0.9426333333333333\n",
      "training accuracy is 0.94565\n",
      "training accuracy is 0.9483333333333334\n",
      "training accuracy is 0.9511166666666667\n",
      "training accuracy is 0.9532333333333334\n",
      "training accuracy is 0.9554\n",
      "training accuracy is 0.9572\n",
      "training accuracy is 0.9586333333333333\n",
      "training accuracy is 0.9602833333333334\n",
      "training accuracy is 0.9615166666666667\n",
      "training accuracy is 0.9628\n",
      "training accuracy is 0.9642166666666667\n",
      "training accuracy is 0.9655666666666667\n",
      "training accuracy is 0.967\n",
      "training accuracy is 0.96825\n",
      "training accuracy is 0.9695333333333334\n",
      "training accuracy is 0.97055\n",
      "training accuracy is 0.9714166666666667\n",
      "training accuracy is 0.97215\n",
      "training accuracy is 0.9732166666666666\n",
      "training accuracy is 0.9740833333333333\n",
      "training accuracy is 0.9751833333333333\n",
      "training accuracy is 0.9761166666666666\n",
      "training accuracy is 0.9769333333333333\n",
      "training accuracy is 0.9775333333333334\n",
      "training accuracy is 0.97845\n",
      "training accuracy is 0.97925\n",
      "training accuracy is 0.9800833333333333\n",
      "training accuracy is 0.9806166666666667\n",
      "training accuracy is 0.9810833333333333\n",
      "training accuracy is 0.9819\n",
      "training accuracy is 0.98275\n",
      "training accuracy is 0.9833666666666666\n",
      "training accuracy is 0.9841166666666666\n",
      "training accuracy is 0.9847\n",
      "training accuracy is 0.9852333333333333\n",
      "training accuracy is 0.9856666666666667\n",
      "training accuracy is 0.9861833333333333\n",
      "training accuracy is 0.9866333333333334\n",
      "training accuracy is 0.9872333333333333\n",
      "training accuracy is 0.9876\n",
      "training accuracy is 0.98815\n",
      "training accuracy is 0.9884666666666667\n",
      "training accuracy is 0.98885\n",
      "training accuracy is 0.98915\n",
      "training accuracy is 0.9894333333333334\n",
      "training accuracy is 0.98975\n",
      "final training accuracy is 0.9900166666666667\n",
      "0.9685\n"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "for epoch in range(epochs):\n",
    "    # fc1: #64-by-#observation \n",
    "    fc1 = F.relu (torch.matmul(W1.double().T,X_train_tensor.T) + b1 ) \n",
    "\n",
    "    y_hat =(torch.matmul(W2.double().T, fc1)+b2).T\n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(y_hat, y_train.long())\n",
    "    \n",
    "    # before loss.backwards()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    ##### getting training accuracy ############\n",
    "    a= (torch.argmax(y_hat,axis=1) - y_train)\n",
    "    train_accuracy =  a[a==0].shape[0] / n_train\n",
    "    if (epoch % 10 == 0) :\n",
    "        print (\"training accuracy is {}\".format(train_accuracy))\n",
    "    if train_accuracy >0.99:\n",
    "        print (\"final training accuracy is {}\".format(train_accuracy))\n",
    "        break\n",
    "#     if epoch % 10 == 0 :\n",
    "#         print('{},\\t{:.2f}'.format(epoch, loss.item()))\n",
    "#         print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W1.grad))))\n",
    "#         print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W2.grad))))\n",
    "#     if ((torch.max(torch.abs((W1.grad))) < 0.001 ) and torch.max(torch.abs((W2.grad))) < 0.001 ) :\n",
    "#         break\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "################### evaluate it on the test set ##########################\n",
    "##########################################################################\n",
    "fc1_test = F.relu (torch.matmul(W1.double().T,X_test_tensor.T) + b1 ) \n",
    "y_test_hat =(torch.matmul( W2.double().T,fc1_test)+b2).T\n",
    "a= (torch.argmax(y_test_hat,axis=1) - y_test)\n",
    "test_accuracy =  a[a==0].shape[0] / n_test\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9688\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "################### evaluate it on the test set ##########################\n",
    "##########################################################################\n",
    "fc1_test = F.relu (torch.matmul(W1.double().T,X_test_tensor.T) + b1 ) \n",
    "y_test_hat =(torch.matmul( W2.double().T,fc1_test)+b2).T\n",
    "a= (torch.argmax(y_test_hat,axis=1) - y_test)\n",
    "test_accuracy =  a[a==0].shape[0] / n_test\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###########################################################################\n",
    "######################## A(5)b ############################################\n",
    "###########################################################################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 32])\n",
      "torch.Size([60000, 784])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "h0 = 32 \n",
    "h1 = 32\n",
    "h2 = 10\n",
    "# Wx+b => W -> [w_0 W], x -> [1;x] \n",
    "W0 = 2/np.sqrt(h0) * torch.rand( h0 , d_train ).T.double() -  1/np.sqrt(h0) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b0 = ones = torch.ones(h0,1).double()\n",
    "W1 = 2/np.sqrt(h1) * torch.rand( h1 , h0 ).double() -  1/np.sqrt(h1) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b1 = ones = torch.ones(h1,1).double()\n",
    "W2 = 2/np.sqrt(h2) * torch.rand( h1 , h2 ).double() -  1/np.sqrt(h2) # uiformly between (-1/sqrt(m), 1/sqrt(m))\n",
    "b2 = ones = torch.ones(h2,1).double()\n",
    "\n",
    "print(W0.shape)\n",
    "print(X_train_tensor.shape)\n",
    "print(W1.shape)\n",
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### separate W and b ######################################\n",
    "epochs=5000\n",
    "learning_rate = .01\n",
    "parms = [W1,b1,W2,b2,W0,b0]#list(list(W2)) #+list(W2))\n",
    "optimizer = torch.optim.Adam(parms, lr=learning_rate)\n",
    "W0.requires_grad = True\n",
    "W1.requires_grad = True\n",
    "W2.requires_grad = True\n",
    "b0.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "b2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is 0.12228333333333333\n",
      "training accuracy is 0.7928\n",
      "training accuracy is 0.8741833333333333\n",
      "training accuracy is 0.9042833333333333\n",
      "training accuracy is 0.9194333333333333\n",
      "training accuracy is 0.93\n",
      "training accuracy is 0.93765\n",
      "training accuracy is 0.94465\n",
      "training accuracy is 0.9485833333333333\n",
      "training accuracy is 0.9528666666666666\n",
      "training accuracy is 0.9565666666666667\n",
      "training accuracy is 0.9596\n",
      "training accuracy is 0.9618833333333333\n",
      "training accuracy is 0.9643\n",
      "training accuracy is 0.9664666666666667\n",
      "training accuracy is 0.9681833333333333\n",
      "training accuracy is 0.96995\n",
      "training accuracy is 0.9715333333333334\n",
      "training accuracy is 0.9730666666666666\n",
      "training accuracy is 0.9743666666666667\n",
      "training accuracy is 0.9756666666666667\n",
      "training accuracy is 0.9769833333333333\n",
      "training accuracy is 0.97835\n",
      "training accuracy is 0.9793833333333334\n",
      "training accuracy is 0.9804166666666667\n",
      "training accuracy is 0.9814666666666667\n",
      "training accuracy is 0.9824\n",
      "training accuracy is 0.9834833333333334\n",
      "training accuracy is 0.98445\n",
      "training accuracy is 0.9852\n",
      "training accuracy is 0.98595\n",
      "training accuracy is 0.9869666666666667\n",
      "training accuracy is 0.9876333333333334\n",
      "training accuracy is 0.9887166666666667\n",
      "training accuracy is 0.9894833333333334\n",
      "final training accuracy is 0.9900666666666667\n",
      "0.9146\n"
     ]
    }
   ],
   "source": [
    "######################### separate W and b ######################################\n",
    "for epoch in range(epochs):\n",
    "    # fc1: #64-by-#observation \n",
    "    fc1 = F.relu (torch.matmul(W0.T.double(),X_train_tensor.T) + b0 ) \n",
    "    # fc1: 32*6000\n",
    "    fc2 = F.relu (torch.matmul(W1.T.double(), fc1)+b1)\n",
    "    \n",
    "    y_hat =(torch.matmul(W2.T.double(), fc2)+b2).T\n",
    "    \n",
    "    # cross entropy combines softmax calculation with NLLLoss\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(y_hat, y_train.long())\n",
    "    \n",
    "    # before loss.backwards()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # computes derivatives of the loss with respect to W\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    ##### getting training accuracy ############\n",
    "    a= (torch.argmax(y_hat,axis=1) - y_train)\n",
    "    train_accuracy =  a[a==0].shape[0] / n_train\n",
    "    if (epoch % 10 == 0) :\n",
    "        print (\"training accuracy is {}\".format(train_accuracy))\n",
    "    if train_accuracy >0.99:\n",
    "        print (\"final training accuracy is {}\".format(train_accuracy))\n",
    "        break\n",
    "#     if epoch % 10 == 0 :\n",
    "#         print('{},\\t{:.2f}'.format(epoch, loss.item()))\n",
    "#         print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W1.grad))))\n",
    "#         print(\"the max of the {} the iteration is {}\".format(epoch,torch.max(torch.abs(W2.grad))))\n",
    "#     if ((torch.max(torch.abs((W1.grad))) < 0.001 ) and torch.max(torch.abs((W2.grad))) < 0.001 ) :\n",
    "#         break\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "################### evaluate it on the test set ##########################\n",
    "##########################################################################\n",
    "\n",
    "fc1_test_2 = F.relu (torch.matmul(W0.double().T,X_test_tensor.T) + b0 ) \n",
    "fc2_test_2 = F.relu (torch.matmul(W1.double().T, fc1_test_2+b1))\n",
    "y_test_hat_2 =(torch.matmul( W2.double().T,fc2_test_2)+b2).T\n",
    "a= (torch.argmax(y_test_hat_2,axis=1) - y_test)\n",
    "test_accuracy =  a[a==0].shape[0] / n_test\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
